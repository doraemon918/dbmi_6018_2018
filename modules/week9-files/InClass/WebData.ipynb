{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pixiedust\n",
    "dataDir = os.path.join(os.getcwd())\n",
    "print (dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urlparse\n",
    "* Methods for breaking web addresses into components\n",
    "* **urlparse**\n",
    "* **urlsplit** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlsplit\n",
    "parse = urlparse('http://www.cwi.nl:80/%7Eguido/Python.html')\n",
    "print( parse.scheme)\n",
    "print (parse.port)\n",
    "print (parse.geturl())\n",
    "print (parse.hostname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2 = urlsplit(\"http://docs.python.org/2/search.html?q=urlparse&check_keywords=yes&area=default\")\n",
    "print (o2.query)\n",
    "print (o2.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib2\n",
    "* Routines for accessing files across a network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import webbrowser\n",
    "fin = urllib.request.urlopen('http://www.utah.edu')\n",
    "pythonTxt = fin.read().decode()\n",
    "output = os.path.join(dataDir,\"utah.html\")\n",
    "f0 = open(output,\"w\")\n",
    "f0.write(pythonTxt)\n",
    "f0.close()\n",
    "\n",
    "# This won't work on bmi6018 but would on your own comptuer\n",
    "#webbrowser.open_new_tab(\"file://\"+output)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output) as f0:\n",
    "    txt = f0.read()\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML files\n",
    "## Beautiful Soup\n",
    "* HTML can be difficult to parse, because the rules for valid HTMl are lax. \n",
    "* [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) cleans up the HTML for you and then returns a comprehensive parse.\n",
    "* Beautiful Soup uses parsers installed with Python\n",
    "* Variety of choices, including built in Python parser. (See [Installing a Parser](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup1 = BeautifulSoup(open(os.path.join(dataDir,\"utah.html\")))\n",
    "#or \n",
    "fin = open(os.path.join(dataDir,\"utah.html\")) # default mode is read\n",
    "data = fin.read()           \n",
    "soup2 = BeautifulSoup(data, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Beautiful Soup transform complex HTML tree into a tree of Python objects\n",
    "* Four Principle Objects\n",
    "    * Tag\n",
    "    * NavigableString\n",
    "    * BeautifulSoup\n",
    "    * Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all the links on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(urllib.request.urlopen(\"http://ncip.nci.nih.gov/blog/future-pathway-interaction-database-pid-conversation-dr-daoud-meerzaman/\"))\n",
    "help(soup3.find_all)\n",
    "for link in soup3.find_all('a'):\n",
    "    print (link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "%%pixie_debugger\n",
    "pageTitles = {}\n",
    "txt = soup3.get_text()\n",
    "links = soup3.find_all('a')\n",
    "hrefs = [link.get('href') for link in links]\n",
    "for href in hrefs:\n",
    "    try:\n",
    "        soup = BeautifulSoup(urllib.request.urlopen(href), \"lxml\")\n",
    "        title = soup.title.string\n",
    "        pageTitles[href] = title\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "keys = list(pageTitles.keys())\n",
    "keys.sort()\n",
    "for key in keys:\n",
    "    print (key,pageTitles[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "h1s = soup3.find_all(re.compile('h[1-6]'))\n",
    "h = h1s[0]\n",
    "\n",
    "for h in h1s:\n",
    "    print (h.name,h.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for tag in soup3.find_all(re.compile(r'^b')):\n",
    "    print (tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = soup3.find_all(href=re.compile(\"nci\\.nih\\.gov\"))\n",
    "for href in hrefs:\n",
    "    print (href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the text on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(urllib.request.urlopen(\"http://www.math.utah.edu/~pa/math/0by0.html\"))\n",
    "#print soup3.get_text()\n",
    "print (soup3.title.string)\n",
    "for tag in soup3.find_all(\"p\"):\n",
    "    for c in tag.children: \n",
    "        print (c.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = os.path.join(dataDir,\"webData.html\")\n",
    "inputAddress = \"file://\"+inputFile\n",
    "soup3 = BeautifulSoup(urllib.request.urlopen(inputAddress))\n",
    "txt = soup3.get_text()\n",
    "for link in soup3.find_all('a'):\n",
    "    print (link.get('href'))\n",
    "for tableRow in soup3.find_all('tr'):\n",
    "    for tableCell in tableRow.children:#.find_all('td'):\n",
    "        if tableCell.name == 'td':\n",
    "            print (tableCell.contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
